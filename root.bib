@book{eliasmith2003neural,
  title={Neural engineering: Computation, representation, and dynamics in neurobiological systems},
  author={Eliasmith, Chris and Anderson, Charles H},
  year={2003},
  publisher={MIT press}
}

@article {eliasmith2012,
  title = {A large-scale model of the functioning brain},
  journal = {Science},
  volume = {338},
  year = {2012},
  pages = {1202-1205},
  doi = {10.1126/science.1225266},
  url = {http://nengo.ca/publications/spaunsciencepaper},
  author = {Chris Eliasmith and Terrence C. Stewart and Xuan Choo and Trevor Bekolay and Travis DeWolf and Yichuan Tang and Daniel Rasmussen},
  pdf = {http://compneuro.uwaterloo.ca/files/publications/eliasmith.2012.pdf}
}

@article{bekolay2013nengo,
  title={Nengo: a Python tool for building large-scale functional brain models},
  author={Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron Russell and Eliasmith, Chris},
  journal={Frontiers in neuroinformatics},
  volume={7},
  year={2013},
  publisher={Frontiers Media SA}
}

@article {stewart2012b,
  title = {The Neural Engineering Framework},
  journal = {AISB Quarterly},
  year = {2012},
  pages = {2-7},
  abstract = {The Neural Engineering Framework (NEF) is a general methodology that allows the building of large-scale, biologically plausible, neural models of cognition. The NEF acts as a neural compiler: once the properties of the neurons, the values to be represented, and the functions to be computed are specified, it solves for the connection weights between components that will perform the desired functions. Importantly, this works not only for feed-forward computations, but also for recurrent connections, allowing for complex dynamical systems including integrators, oscillators, Kalman filters, etc. The NEF also incorporates realistic local error-driven learning rules, allowing for the online adaptation and optimisation of responses. The NEF has been used to model visual attention, inductive reasoning, reinforcement learning and many other tasks. Recently, we used it to build Spaun, the world{\textquoteright}s largest functional brain model, using 2.5 million neurons to perform eight different cognitive tasks by interpreting visual input and producing hand-written output via a simulated 6-muscle arm.  Our open-source software Nengo was used for all of these, and is available at http://nengo.ca, along with tutorials, demos, and downloadable models.
},
  author = {Terrence C. Stewart},
  pdf = {http://compneuro.uwaterloo.ca/files/publications/stewartAISB.2012.pdf}
}

@inproceedings{Rasmussen2014b,
address = {Austin},
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
editor = {Bello, Paul and Guarini, Marcello and McShane, Marjorie and Scassellati, Brian},
pages = {1252--1257},
publisher = {Cognitive Science Society},
title = {A neural model of hierarchical reinforcement learning},
year = {2014},
pdf={https://mindmodeling.org/cogsci2014/papers/221/paper221.pdf},
url={https://mindmodeling.org/cogsci2014/papers/221/index.html}
}

@article {dewolf2011,
  title = {The neural optimal control hierarchy for motor control},
  journal = {The Journal of Neural Engineering},
  volume = {8},
  year = {2011},
  month = {11/2011},
  pages = {21},
  abstract = {Our empirical, neuroscientific understanding of biological motor systems has been rapidly growing in recent years. However, this understanding has not been systematically mapped to a quantitative characterization of motor control based in control theory. Here, we attempt to bridge this gap by describing the neural optimal control hierarchy (NOCH), which can serve as a foundation for biologically plausible models of neural motor control. The NOCH has been constructed by taking recent control theoretic models of motor control, analyzing the required processes, generating neurally plausible equivalent calculations and mapping them on to the neural structures that have been empirically identified to form the anatomical basis of motor control. We demonstrate the utility of the NOCH by constructing a simple model based on the identified principles and testing it in two ways. First, we perturb specific anatomical elements of the model and compare the resulting motor behavior with clinical data in which the corresponding area of the brain has been damaged. We show that damaging the assigned functions of the basal ganglia and cerebellum can cause the movement deficiencies seen in patients with Huntington's disease and cerebellar lesions. Second, we demonstrate that single spiking neuron data from our model's motor cortical areas explain major features of single-cell responses recorded from the same primate areas. We suggest that together these results show how NOCH-based models can be used to unify a broad range of data relevant to biological motor control in a quantitative, control theoretic framework.},
  url = {http://iopscience.iop.org/1741-2552/8/6/065009},
  author = {Travis DeWolf and Chris Eliasmith},
  keywords = {motor control, NOCH, optimal control, hierarchy, basal ganglia, cerebellum, motor cortex},
  pdf = {http://compneuro.uwaterloo.ca/files/publications/dewolf.2011.pdf}
}

@article {conklin2005,
  title = {A controlled attractor network model of path integration in the rat},
  journal = {Journal of Computational Neuroscience},
  volume = {18},
  year = {2005},
  pages = {183-203},
  author = {John Conklin and Chris Eliasmith},
  pdf = {http://compneuro.uwaterloo.ca/files/publications/conklin.2005.pdf}
}

@article {tripp2010,
	title = {Population models of temporal differentiation},
	journal = {Neural Computation},
	volume = {22},
	year = {2010},
	pages = {621-659},
	author = {Bryan Tripp and Chris Eliasmith},
    pdf = {http://compneuro.uwaterloo.ca/files/publications/tripp.2010.pdf}
}

@mastersthesis {huzook2012,
  title = {A mechanistic model of motion processing in the early visual system},
  volume = {Master of Applied Science},
  year = {2012},
  month = {12/2012},
  pages = {94},
  school = {University of Waterloo},
  type = {Masters Thesis},
  address = {Waterloo, Ontario},
  abstract = {A prerequisite for the perception of motion in primates is the transformation of varying intensities of light on the retina into an estimation of position, direction and speed of coherent objects. The neuro-computational mechanisms relevant for object feature encoding have been thoroughly explored, with many neurally plausible models able to represent static visual scenes. However, motion estimation requires the comparison of successive scenes through time. Precisely how the necessary neural dynamics arise and how other related neural system components interoperate have yet to be shown in a large-scale, biologically realistic simulation.
The proposed model simulates a spiking neural network computation for representing object velocities in cortical areas V1 and middle temporal area (MT). The essential neural dynamics, hypothesized to reside in networks of V1 simple cells, are implemented through recurrent population connections that generate oscillating spatiotemporal tunings. These oscillators produce a resonance response when stimuli move in an appropriate manner in their receptive fields. The simulation shows close agreement between the predicted and actual impulse responses from V1 simple cells using an ideal stimulus.
By integrating the activities of like V1 simple cells over space, a local measure of visual pattern velocity can be produced. This measure is also the linear weight of an associated velocity in a retinotopic map of optical flow. As a demonstration, the classic motion stimuli of drifting sinusoidal gratings and variably coherent dots are used as test stimuli and optical flow maps are generated. Vector field representations of this structure may serve as inputs for perception and decision making processes in later brain areas.},
  keywords = {large-scale spiking model, oscillator interference, visual motion},
  url = {http://hdl.handle.net/10012/7140},
  author = {Aziz Hurzook},
  pdf = {http://compneuro.uwaterloo.ca/files/publications/hurzook.2012.pdf}
}

@article{russell1995modern,
  title={A modern approach},
  author={Russell, Stuart and Norvig, Peter and Intelligence, Artificial},
  journal={Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs},
  volume={25},
  year={1995},
  publisher={Citeseer}
}

@article{hinson1983matching,
  title={MATCHING, MAXIMIZING, AND HILL-CLIMBING},
  author={Hinson, John M and Staddon, JER},
  journal={Journal of the Experimental Analysis of Behavior},
  volume={40},
  number={3},
  pages={321--331},
  year={1983},
  publisher={Wiley Online Library}
}

@inproceedings{yuret1993dynamic,
  title={Dynamic hill climbing: Overcoming the limitations of optimization techniques},
  author={Yuret, Deniz and De La Maza, Michael},
  booktitle={The Second Turkish Symposium on Artificial Intelligence and Neural Networks},
  pages={208--212},
  year={1993},
  organization={Citeseer}
}

@article{bayesianstructure, 
year={2006},
issn={0885-6125},
journal={Machine Learning},
volume={65},
number={1},
doi={10.1007/s10994-006-6889-7},
title={The max-min hill-climbing Bayesian network structure learning algorithm},
url={http://dx.doi.org/10.1007/s10994-006-6889-7},
publisher={Kluwer Academic Publishers},
keywords={Bayesian networks; Graphical models; Structure learning},
author={Tsamardinos, Ioannis and Brown, LauraE. and Aliferis, ConstantinF.},
pages={31-78},
language={English}
}

@inproceedings{chalup1999study,
  title={A study on hill climbing algorithms for neural network training},
  author={Chalup, Stephan and Maire, Frederic},
  booktitle={Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on},
  volume={3},
  year={1999},
  organization={IEEE}
}

@inproceedings{kimura1995reinforcement,
  title={Reinforcement learning by stochastic hill climbing on discounted reward},
  author={Kimura, Hajime},
  booktitle={Proceedings of the 12th International Conference on Machine Learning},
  pages={295--303},
  year={1995}
}